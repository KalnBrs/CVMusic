import cv2
import json
import os
import numpy as np

# Global variables for mouse callback
points = []
img_display = None

def click_event(event, x, y, flags, param):
    global points, img_display
    if event == cv2.EVENT_LBUTTONDOWN:
        points.append((x, y))
        # Visual feedback
        cv2.circle(img_display, (x, y), 10, (0, 0, 255), -1)
        cv2.putText(img_display, str(len(points)), (x+10, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
        cv2.imshow("Calibration", img_display)

def setup_interactive():
    global img_display, points
    
    ref_path = "Backend/reference_neck.jpg"
    if not os.path.exists(ref_path):
        ref_path = "reference_neck.jpg"
        
    if not os.path.exists(ref_path):
        print(f"âŒ æ‰¾ä¸åˆ°åŸºå‡†å›¾: {ref_path}")
        print("è¯·å…ˆè¿è¡Œ generate_reference_grid.py ç”ŸæˆåŸºå‡†å›¾ (å“ªæ€• grid ä¸å‡†ä¹Ÿæ²¡å…³ç³»ï¼Œæˆ‘ä»¬éœ€è¦å›¾)")
        return

    img = cv2.imread(ref_path)
    img_display = img.copy()
    h, w = img.shape[:2]
    
    print("========================================================")
    print("ğŸ¸ äº¤äº’å¼å‰ä»–æŒ‡æ¿æ ‡å®šå·¥å…·")
    print("========================================================")
    print("è¯·åœ¨å¼¹å‡ºçš„çª—å£ä¸­ï¼Œä¾æ¬¡ç‚¹å‡»æŒ‡æ¿çš„ 4 ä¸ªè§’è½ï¼š")
    print("1. ã€ç´æ• (0å“) - ä¸Šæ–¹ã€‘ (ç¬¬1å¼¦, é«˜éŸ³å¼¦)")
    print("2. ã€ç´æ• (0å“) - ä¸‹æ–¹ã€‘ (ç¬¬6å¼¦, ä½éŸ³å¼¦)")
    print("3. ã€ç´èº«ç«¯ (é«˜å“) - ä¸Šæ–¹ã€‘ (ç¬¬1å¼¦, é«˜éŸ³å¼¦)")
    print("4. ã€ç´èº«ç«¯ (é«˜å“) - ä¸‹æ–¹ã€‘ (ç¬¬6å¼¦, ä½éŸ³å¼¦)")
    print("--------------------------------------------------------")
    print("ç‚¹å‡»æ»¡ 4 ä¸ªç‚¹åï¼ŒæŒ‰ä»»æ„é”®å®Œæˆã€‚")
    print("å¦‚æœç‚¹é”™äº†ï¼Œè¯·é‡æ–°è¿è¡Œè„šæœ¬ã€‚")
    
    cv2.namedWindow("Calibration", cv2.WINDOW_NORMAL)
    # Resize window if image is huge (4K)
    if w > 1800:
        cv2.resizeWindow("Calibration", 1600, int(1600 * h / w))
        
    cv2.setMouseCallback("Calibration", click_event)
    cv2.imshow("Calibration", img_display)
    cv2.waitKey(0)
    cv2.destroyAllWindows()
    
    if len(points) != 4:
        print(f"âŒ ä½ ç‚¹å‡»äº† {len(points)} ä¸ªç‚¹ï¼Œéœ€è¦æ­£å¥½ 4 ä¸ªç‚¹ã€‚è¯·é‡è¯•ã€‚")
        return
        
    print(f"âœ… æ•è·åæ ‡: {points}")
    
    # Sort points to ensure logical mapping regardless of click order, 
    # BUT we trust the user followed instructions 1->2->3->4 better.
    # Let's assume 1:TL, 2:BL, 3:TR, 4:BR (Left=Nut, Right=Body)
    # OR 1:TR, 2:BR, 3:TL, 4:BL (Right=Nut, Left=Body)
    # Based on tracked_chordD.jpg, Nut is on the RIGHT.
    # So "Nut" points should have larger X than "Body" points.
    
    p1, p2, p3, p4 = points
    
    # Let's sort by X to determine orientation
    avg_x_12 = (p1[0] + p2[0]) / 2
    avg_x_34 = (p3[0] + p4[0]) / 2
    
    nut_points = []
    body_points = []
    
    if avg_x_12 > avg_x_34:
        # 1,2 are Nut (Right), 3,4 are Body (Left)
        print("æ£€æµ‹åˆ°ç´å¤´åœ¨ã€å³ä¾§ã€‘")
        nut_points = [p1, p2]
        body_points = [p3, p4]
    else:
        # 1,2 are Nut (Left), 3,4 are Body (Right)
        print("æ£€æµ‹åˆ°ç´å¤´åœ¨ã€å·¦ä¾§ã€‘")
        nut_points = [p1, p2] # Actually these are Nut
        body_points = [p3, p4]
        
    # Sort by Y to separate String 1 (Top) and String 6 (Bottom)
    # Smaller Y is Top
    nut_points.sort(key=lambda p: p[1])
    body_points.sort(key=lambda p: p[1])
    
    # Now we have explicit corners
    nut_top = nut_points[0]    # String 1 Nut
    nut_bot = nut_points[1]    # String 6 Nut
    body_top = body_points[0]  # String 1 Body
    body_bot = body_points[1]  # String 6 Body
    
    # Generate Grid
    grid_data = {
        "strings": {},
        "fret_lines": []
    }
    
    # 1. Interpolate Strings
    # We have 6 strings.
    for i in range(1, 7): # 1..6
        t = (i - 1) / 5.0
        # Interpolate start point (at Nut)
        sx = nut_top[0] + t * (nut_bot[0] - nut_top[0])
        sy = nut_top[1] + t * (nut_bot[1] - nut_top[1])
        
        # Interpolate end point (at Body)
        ex = body_top[0] + t * (body_bot[0] - body_top[0])
        ey = body_top[1] + t * (body_bot[1] - body_top[1])
        
        # Extend the lines slightly to cover full image width if needed?
        # Better to keep them strictly within the clicked region for accuracy.
        grid_data["strings"][str(i)] = [
            [int(sx), int(sy)],
            [int(ex), int(ey)]
        ]
        
    # 2. Generate Frets
    # We need Scale Length.
    # Distance from Nut to 12th Fret is Scale/2.
    # Did the user click the 12th fret as the "Body" point? 
    # Usually users click the end of the image.
    # Let's assume the "Body Points" are roughly around Fret 12-15.
    # Let's dynamically estimate Scale based on visual length assuming the clicked region is ~12 frets.
    # A standard neck join is at 14th fret. Let's assume the user clicked near the 14th fret area.
    
    # Calculate pixel length of the board
    len_top = np.linalg.norm(np.array(nut_top) - np.array(body_top))
    len_bot = np.linalg.norm(np.array(nut_bot) - np.array(body_bot))
    avg_len = (len_top + len_bot) / 2
    
    # Formula: dist_n = Scale * (1 - 2^(-n/12))
    # If avg_len corresponds to fret N (e.g. 14), then:
    # avg_len = Scale * (1 - 2^(-14/12))
    # Scale = avg_len / (1 - 2^(-14/12))
    ASSUMED_LAST_FRET = 14
    factor = 1 - (2 ** (-ASSUMED_LAST_FRET / 12.0))
    scale_px = avg_len / factor
    
    print(f"æ¨ç®— Scale: {scale_px:.1f} px (å‡è®¾ä½ ç‚¹çš„æ˜¯ç¬¬ {ASSUMED_LAST_FRET} å“)")
    
    # Generate 20 fret lines
    for n in range(0, 21):
        ratio = (1 - (2 ** (-n / 12.0)))
        dist = scale_px * ratio
        
        # We need to interpolate along the "fretboard axis"
        # Vector from Nut to Body
        vec_top = np.array(body_top) - np.array(nut_top)
        vec_bot = np.array(body_bot) - np.array(nut_bot)
        
        # Normalize vectors is not needed, we use ratio of the TOTAL clicked length
        # But wait, the total clicked length is only up to ASSUMED_LAST_FRET
        # So dist is pixels from Nut.
        # We need to map "pixels from Nut" to "t (0..1) along the clicked quad"
        
        # t = dist / avg_len  (Roughly)
        t = dist / avg_len
        
        # Clamp? No, we want to draw even if it goes beyond clicked area
        
        p_top = np.array(nut_top) + vec_top * t
        p_bot = np.array(nut_bot) + vec_bot * t
        
        grid_data["fret_lines"].append([
            p_top.tolist(),
            p_bot.tolist()
        ])
        
    # Save
    out_path = "Backend/reference_grid.json"
    if not os.path.exists("Backend"):
        out_path = "reference_grid.json"
        
    with open(out_path, "w") as f:
        json.dump(grid_data, f, indent=2)
        
    print("âœ… æ ¡å‡†å®Œæˆï¼reference_grid.json å·²æ›´æ–°ã€‚")
    print("ç°åœ¨è¯·é‡æ–°è¿è¡Œ test_pictures2_fingers.py éªŒè¯æ•ˆæœã€‚")

if __name__ == "__main__":
    setup_interactive()

